\section{Memo}\label{sec:memorandom}
\newcommand{\rmapr}{M_{\mybf{R}}\mybf{R}}
\newcommand{\nmapr}{M_{\mybf{R}}\mybf{N}}
\newcommand{\zmapr}{M_{\mybf{R}}\mybf{Z}}
\newcommand{\loner}{L\mybf{R}}
\newcommand{\intallr}[1]{\int_{{#1}\in\mybf{R}}}

\newcommand{\sgraph}{\entrymodifiers={++[o][F-]}\xymatrix@R=1pt@C-1pc}

\subsection{音楽CDの売り上げ推移}
\begin{itemize}
	\item 熱心なファンが存在する
	\item 熱心なファンはCDが発売されたら必ず買う
	\item 熱心なファンはCDの発売を待ち焦がれている
\end{itemize}
次のようなモデルを考える。
\begin{equation}\xymatrix{
	\myop{fan} \ar[r]^k & \myop{bought} \\
}\end{equation}
\begin{equation}\begin{split}
	\partial_tx &= -Kx \\
	x &= \begin{pmatrix}
		\myop{fan} \\
		\myop{bought} \\
		\end{pmatrix} \\
	K &= kM \\
	M &= \begin{pmatrix}
		1 & 0 \\
		- 1 & 0 \\
		\end{pmatrix} \\
	k &\ge 0 \\
\end{split}\end{equation}
$M$がべき等$M^2=M$だから、この微分方程式の解は次のようになる。
\begin{equation}\begin{split}
	x(t) &= e^{-Kt}x_0 \\
		&= x_0 + (e^{-kt}-1)Mx_0 \\
	\myop{fan}(t) &= e^{-kt}\myop{fan}_0 \\
	\myop{bought}(t) &= \myop{bought}_0 + (1 - e^{-kt})\myop{fan}_0 \\
\end{split}\end{equation}
実際には、流れ係数は
\begin{itemize}
	\item 発売日、
	\item 曜日、
	\item 祝祭日やクリスマスなどの催事、
	\item 販売促進のためのメディア出演、
\end{itemize}
などに依存するので、時間に関して定数とはいえない。
したがって、一日単位で流れ係数が異なるとすると、一日単位で積分して、
購買者数$\myop{bought}$は次のような差分方程式で表される。
\begin{equation}\begin{split}
	\myop{fan}_{n+1} &= a_{n+1}\myop{fan}_n \\
	\myop{bought}_{n+1} &= \myop{bought}_n + (1 - a_{n+1})\myop{fan}_n \\
	a_{n} &= e^{- k_n\delta} \\
\end{split}\end{equation}
ここで、$\delta$は一日の時間である。一日を単位とした場合は、$\delta=1$である。
したがって、一日の売り上げ$\myop{sales}$は次のようになる。
\begin{equation}\begin{split}
	\myop{fan}_{n+1} &= a_{n+1}\myop{fan}_n \\
	\myop{sales}_{n+1} &= \myop{bought}_{n+1} - \myop{bought}_n \\
		&= (1 - a_{n+1})\myop{fan}_n \\
\end{split}\end{equation}
売り上げの結果から流れ係数を推定するのに便利な形に直すと次のようになる。
\begin{equation}\begin{split}
	a_{n+1} &= 1 - \frac{\myop{sales}_{n+1}}{\myop{fan}_n} \\
		&= 1 - \frac{\myop{sales}_{n+1}}{\myop{fan}_0 - \sum_{k=1}^n\myop{sales}_k} \\
		&= 1 - \frac{\myop{sales}_{n+1}}{\myop{fan}_0 + \myop{bought}_0 - \myop{bought}_n} \\
\end{split}\end{equation}

\subsection{モンテカルロ法}
何故、モンテカルロ法は積分を近似するのだろうか。
実数の区間$[a,b]$で定義された関数$f$に対するモンテカルロ法による
近似は次のように書ける。
\begin{equation}\begin{split}
	\int_a^bdxf(x) \simeq \frac{b-a}{n}\sum_{i=1}^nf(x_i) \\
\end{split}\end{equation}
ここで、$\set{x_1,\dots,x_n}$はモンテカルロ法で生成された点列である。

有限集合で考えてみる。
$X$を有限集合、$PX$を$X$の分布全体のなす空間とする。
\begin{equation}\begin{split}
	PX &= \set{X\to \mybf{R}^{\zettai{X}}\bou\text{below}} \\
	& p(x)\in [0,1] \text{ for all }p\in PX, x\in X \text{ and }\\
	& \sum_{x\in X} p(x)=1 \\
\end{split}\end{equation}
最終分布$p\in PX$が与えられたする。
任意の初期分布から出発して、最終分布$p$に達して定常分布になる
遷移確率$T$を求めてみる。$p$への射影$\pi_p$は求める遷移確率$T$となる。
\begin{equation}\begin{split}
	\pi_p &= \tau p \\
	\pi_p q &= p\myop{tr}(q) \text{ for all }q\in PX \\
\end{split}\end{equation}

三つの状態$X=\set{x_1,x_2,x_3}$の最終分布を$(p_1,p_2,p_3)\in PX$とする。

確率ベクトルの空間$P$を考える。
\begin{itemize}
	\item 終状態$p_\infty\in P$が与えられる。
	\item 任意の始状態$p_0\in P$に対して、
	$p_\infty=T_\infty p_0$となる遷移行列$T_\infty$を求める。
	遷移行列$T_\infty$はべき等$T_\infty^2=T_\infty$となる。
	\item どうやって遷移確率$T_\infty$を求めるか。
	\item $p_\infty$への射影が$T_\infty$となる。
	$u^t=(1, 1, \dots , 1)$とすると、$T_\infty=T=p_\infty u^t$と書ける。
	$T_\infty$には逆行列は存在しない。
	なぜなら、$T_\infty=p_\infty u^t$だから、
	\begin{equation}\begin{split}
		T_\infty &= \begin{pmatrix}
			p_\infty(1) & \cdots & p_\infty(1) \\
			\vdots & \vdots & \vdots \\
			p_\infty(n) & \cdots & p_\infty(n) \\
			\end{pmatrix} \\
	\end{split}\end{equation}
	という形になり、各列が等しくなり行列式が$0$になる。
\end{itemize}

$\mybf{R}$上のベクトル空間$V$とその線形変換$M$に対して、
成分毎の掛け算$\odot$を定義する。
\begin{equation}\begin{split}
	\odot: V\times V &\to V \\
		\begin{pmatrix}x_1\\ \vdots\\ x_n\end{pmatrix}
		\times \begin{pmatrix}y_1\\ \vdots\\ y_n\end{pmatrix}
		&\mapsto \begin{pmatrix}x_1y_1\\ \vdots\\ x_ny_n\end{pmatrix} \\
	\odot: M\times M &\to M \\
	\begin{pmatrix}
		x_{11} & & x_{1n} \\ 
		\vdots & \vdots & \vdots \\ 
		x_{n1} & & x_{nn} \\ 
	\end{pmatrix}
	\times \begin{pmatrix}
		y_{11} & & y_{1n} \\ 
		\vdots & \vdots & \vdots \\ 
		y_{n1} & & y_{nn} \\ 
	\end{pmatrix}
	&\mapsto \begin{pmatrix}
		x_{11}y_{11} & & x_{1n}y_{1n} \\ 
		\vdots & \vdots & \vdots \\ 
			x_{n1}y_{n1} & & x_{nn}y_{nn} \\ 
		\end{pmatrix}
\end{split}\end{equation}
二項演算$\odot$は可換モノイドとなり、その単位元$1_\odot$は次のようになる。
\begin{equation}\begin{split}
	1_\odot &= \begin{pmatrix}
		1 \\
		1 \\
		\vdots \\
		1 \\
		\end{pmatrix} \\
	1_\odot &= \begin{pmatrix}
		1 & 1 & \cdots & 1 \\
		1 & 1 & \cdots & 1 \\
		\vdots & \vdots & \vdots & \vdots \\
		1 & 1 & \cdots & 1 \\
		\end{pmatrix} \\
\end{split}\end{equation}
また、加法$+$との分配則を満たすので、組$(+,\odot)$は半環となる。
$V$から$M$への写像$\tau$を次のように定義する。
\begin{equation}\begin{split}
	\tau: V &\to M \\
		\begin{pmatrix}
			x_1 \\ 
			x_2 \\ 
			\vdots \\ 
			x_n \\
		\end{pmatrix} &\mapsto 
		\begin{pmatrix}
			x_1 & x_1 & \dots & x_1 \\ 
			x_2 & x_2 & \dots & x_2 \\ 
			\vdots & \vdots & \vdots & \vdots \\ 
			x_n & x_n & \dots & x_n \\ 
		\end{pmatrix} 
\end{split}\end{equation}
写像$\tau$は$+$と$\odot$に対して準同型となり、
半環$(V,+,\odot)$から半環$(M,+,\odot)$への半環準同型となる。
列の和を定義する。

行列のトレースにならって、$V$にもトレースを定義する。
\begin{equation}\begin{split}
	\myop{tr}: V &\to \mybf{R} \\
		\begin{pmatrix}
			x_1 \\ 
			x_2 \\ 
			\vdots \\ 
			x_n \\
		\end{pmatrix} &\mapsto x_1+x_2+\cdots+x_n \\
\end{split}\end{equation}
すると、任意の$v\in V$に対して$\myop{tr}(v)=\myop{tr}\kakko{\tau\kakko{v}}$
が成り立つ。
任意の$v,w\in V$に対して、$\tau(v)\tau(w)=\tau(v)\myop{tr}(w)$が、
任意の$v,w\in V$に対して、$\tau(v)\tau(w)=\tau(v)\myop{tr}(w)$が、
成り立つ。
\begin{equation}\begin{split}
		\begin{pmatrix}
			x_1 & \dots & x_1 \\ 
			\vdots & \vdots & \vdots \\ 
			x_n & \dots & x_n \\ 
		\end{pmatrix} \begin{pmatrix}
			y_1 & \dots & y_1 \\ 
			\vdots & \vdots & \vdots \\ 
			y_n & \dots & y_n \\ 
		\end{pmatrix} &= \begin{pmatrix}
			x_1 & \dots & x_1 \\ 
			\vdots & \vdots & \vdots \\ 
			x_n & \dots & x_n \\ 
		\end{pmatrix}(y_1+\cdots+y_n) \\
\end{split}\end{equation}

\subsection{プログラミング言語}
\subsubsection{多重継承}
コード量を減らすためにも多重継承の機能はあった方がよい。
多重継承の利点と不利点は次のようになるだろう。
\begin{itemize}
\item 利点
	\begin{itemize}
	\item コード量が少なくなる
	\end{itemize}
\item 不利点
	\begin{itemize}
	\item 継承関係が複雑になる
	\item 名前の衝突が起こる
	\item 処理系が複雑になる
	\end{itemize}
\end{itemize}

\subsection{KMP method (Kunuth-Morris-Pratt method)}
KMP is the algorithm that find occurence of a word within an another word.
We shall observe some examples before introducing KMP algorithm.

\begin{observe}
Let consider the problem to find matching part of the pattern word 'ABCD' 
in the input word 'ABxCD...'. At the first try, we find that 'AB' matches
and 'C' dose not match with from the begining of input string.
\begin{equation}\label{eq:kpm-first}\begin{array}{ccc|cccc}
	A&B&x&C&D&\cdots \\
	A&B&C&D \\
\end{array}\end{equation}
Then we can try to find matching from the position of 'C' of the input word
at next try: 
\begin{equation}\label{eq:kpm-next}\begin{array}{cccc|ccc}
	A&B&x&C&D&\cdots\\
	&&&A&B&C&D \\
\end{array}\end{equation}
. We dont need to test the following cases:
\begin{equation}\begin{array}{cc|ccccc}
	A&B&x&C&D&\cdots\\
	&A&B&C&D \\
\end{array}\end{equation}
\begin{equation}\begin{array}{ccc|cccc}
	A&B&x&C&D&\cdots\\
	&&A&B&C&D \\
\end{array}\end{equation}
, because we already know at the first try\eqref{eq:kpm-first} that 
matching is never occured in the above cases.
The state transtion of the pattern 'ABCD' is written as the follwings:
\begin{equation}\begin{array}{rcrcrcr}
	\bakko{ABCD} &=& A\bakko{BCD} &+& 0 &+& A^c \bakko{ABCD} \\
	\bakko{BCD} &=& B\bakko{CD} &+& A\bakko{BCD} &+& (A+B)^c \bakko{ABCD} \\
	\bakko{CD} &=& C\bakko{D} &+& A\bakko{BCD} &+& (A+C)^c \bakko{ABCD} \\
	\bakko{D} &=& D\bakko{} &+& A\bakko{BCD} &+& (A+D)^c \bakko{ABCD} \\
\end{array}\end{equation}
. Where we denote as the followings:
\begin{itemize}
	\item $\bakko{a_1a_2\cdots}$ as a word
	\item $+$ as the union of sets
	\item $\cdot$ as the intersect of sets
	\item $c$ as the complement of set
\end{itemize}
.
\end{observe}

\begin{observe}
Let consider the problem to find matching part of the pattern word 'ABCDABD' 
in the input word 'ABCDABCDABD...'.
We find being matched until the second 'C' in the input word.
\begin{equation}\begin{array}{ccccccc|ccccc}
	A&B&C&D&A&B&C&D&A&B&D&\cdots \\
	A&B&C&D&A&B&D \\
\end{array}\end{equation}
Then we find being matched from the second 'A' to the third 'D' in the
input word.
\begin{equation}\begin{array}{ccccccccccc|c}
	A&B&C&D&A&B&C&D&A&B&D&\cdots \\
	&&&&A&B&C&D&A&B&D \\
\end{array}\end{equation}
The state transtion of the pattern 'ABCDABD' is written as the follwings:
\begin{equation}\begin{split}
	\begin{array}{r}
		\bakko{ABCDABD} \\
		\bakko{BCDABD} \\
		\bakko{CDABD} \\
		\bakko{DABD} \\
		\bakko{ABD} \\
		\bakko{BD} \\
		\bakko{D} \\
		\bakko{} \\
	\end{array} &\mapsto \kakko{\begin{array}{rrrrrrrrrr}
		A^c  & A & 0 & 0 & 0 & 0 & 0 & 0 \\
		(A+B)^c  & A & B & 0 & 0 & 0 & 0 & 0 \\
		(A+C)^c  & A & 0 & C & 0 & 0 & 0 & 0 \\
		(A+C+D)^c  & A & 0 & C & D & 0 & 0 & 0 \\
		A^c  & 0 & 0 & 0 & 0 & A & 0 & 0 \\ 
		(A+B)^c  & A & 0 & 0 & 0 & 0 & B & 0 \\
		(A+C+D)^c  & A & 0 & C & 0 & 0 & 0 & D \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
	\end{array}} \begin{array}{r}
		\bakko{ABCDABD} \\ 
		\bakko{BCDABD} \\ 
		\bakko{CDABD} \\ 
		\bakko{DABD} \\
		\bakko{ABD} \\
		\bakko{BD} \\
		\bakko{D} \\
		\bakko{} \\
	\end{array} \\
\end{split}\end{equation}
.
\end{observe}

\begin{observe}
Let consider the problem to find matching part of the pattern word 'AABAABAAA'
in some input word. The pattern word is indexed as the follwings:
\begin{equation}\begin{array}{ccccccccc}
	0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
	A & A & B & A & A & B & A & A & A \\
\end{array}\end{equation}
. Let examine next index of the case 8-th alphabet being not-matched.
The sub patterns within the pattern words 'AABAABAAA' and 'AABAAB'
are the followings:
\begin{equation}\begin{split}
	\contraction{}{A_0}{A_1B_2A_3}{A_4}
	\contraction[2ex]{A_0A_1B_2}{A_3}{A_4B_5A_6}{A_7}
	&A_0A_1B_2A_3A_4B_5A_6A_7\underline{A_8} \\
	\contraction{}{A_0}{}{A_1}
	\contraction{A_0A_1B_2}{A_3}{}{A_4}
	&A_0A_1B_2A_3A_4\underline{B_5} \\
\end{split}\end{equation}
, then psuedo-code is written as the follwings:
\begin{cprog}
	/**
	 * finds end indices that word_1 matches within word_0.
	 */
	next(word_0: S[], index_0: Index, word_1: S[], index_1: Index): Index {
		if word_0.length() <= index_0 | word_1.length() <= index_1 {
			return (index_0, index_1);
		} else word_0(index_0) == word(index_1) {
			return next(word_0, index_0 += 1, word_1, index_1 += 1);
		} else index_1 == 0 {
			return next(word_0, index_0 += 1, word_1, 0);
		} else index_1 == 2 {
			return next(word_0, index_0, word_1, 0);
		} else index_1 == 5 {
			//return next(word_0, index_0, word_1, 2);
			return next(word_0, index_0, word_1, 0);
		} else index_1 == 8 {
			return next(word_0, index_0, word_1, 5);
		} else {
			...
		}
	}
\end{cprog}
. The backtracking is written in the above psued-code only 
in the case of index\_1 being $\set{2,5,8}$. 
\end{observe}

\begin{equation}\begin{split}
	\begin{pmatrix}
		y_1 \\
		x_1 \\
	\end{pmatrix} &= \begin{pmatrix}
		A & 1 \\
		0 & 1 \\
	\end{pmatrix} \rhd \begin{pmatrix}
		y_1 \\
		x_1 \\
	\end{pmatrix} \\
	\begin{pmatrix}
		x_1 \\
		x_2 \\
		\vdots \\
		x_n \\
	\end{pmatrix} &= \begin{pmatrix}
		0 & a_1 & \cdots & 0 \\
		0 & 0 & \cdots & 0 \\
		\vdots \\
		0 & 0 & \cdots & 1 \\
	\end{pmatrix} \rhd \begin{pmatrix}
		x_1 \\
		x_2 \\
		\vdots \\
		x_n \\
	\end{pmatrix} \\
	\begin{pmatrix}
		y_1 \\
		x_1 \\
		x_2 \\
		\vdots \\
		x_n \\
	\end{pmatrix} &= \begin{pmatrix}
		0 & a_1 & \cdots & 0 \\
		0 & 0 & \cdots & 0 \\
		\vdots \\
		0 & 0 & \cdots & 1 \\
	\end{pmatrix} \rhd \begin{pmatrix}
		y_1 \\
		x_1 \\
		x_2 \\
		\vdots \\
		x_n \\
	\end{pmatrix} \\
\end{split}\end{equation}

Let perform subset construction $D$ for the word $A^*a_1\cdots a_n$
\begin{equation}\begin{split}
	D\bakko{A^*}\bakko{a_1\cdots a_n} &= D\kakko{\bakko{}+\bakko{AA^*}}\bakko{a_1\cdots a_n} \\
		&= a_1\bakko{a_2\cdots a_n} + A\bakko{A^*}\bakko{a_1\cdots a_n} \\
		&= a_1\kakko{\bakko{a_2\cdots a_n}+\bakko{A^*}\bakko{a_1\cdots a_n}} + (\neg a_1)\bakko{A^*}\bakko{a_1\cdots a_n} \\
\end{split}\end{equation}

\subsection{Subset construction}
We denote the subset construction in the following forms:
\begin{equation}\begin{split}
	\varphi\alpha &= \sgraph {
		-\infty \ar[r] & \alpha \ar[r] & \infty \\
	} \\
\end{split}\end{equation}
. $\varphi$ satisfies the followings:
\begin{equation}\begin{split}
	\varphi(\alpha + \beta) &= \kakko{\sgraph {
		-\infty \ar[r] & \alpha \ar[r] & \infty \\
	}} + \kakko{\sgraph {
		-\infty \ar[r] & \beta \ar[r] & \infty \\
	}} \\
	&= \sgraph {
		& \alpha \ar[dr] & \\
		-\infty \ar[ur]\ar[dr] & & \infty \\
		& \beta \ar[ur] & \\
	} \\
	\varphi(\alpha * \beta) &= \kakko{\sgraph {
		-\infty \ar[r] & \alpha \ar[r] & \infty \\
	}} * \kakko{\sgraph {
		-\infty \ar[r] & \beta \ar[r] & \infty \\
	}} \\
	&= \sgraph {
		-\infty \ar[r] & \alpha \ar[r] & \beta \ar[r] & \infty \\
	} \\
	\varphi(\alpha^*) &= \kakko{\sgraph {
		-\infty \ar[r] & \alpha \ar[r] & \infty \\
	}}^* \\
	&= \sgraph {
		-\infty \ar[r] & \alpha \ar[r] \ar@(ur,ul) & \infty \\
	} \\
	\varphi\bakko{a} &= \sgraph {
		-\infty \ar[r] & \bakko{a} \ar[r] & \infty \\
	} \\
	\varphi1 &= \sgraph { -\infty \ar[r] & \infty } \\
	\varphi0 &= 0 \\
\end{split}\end{equation}
. 

\subsection{Backup}
We denote $\mybf{B}$ as the Boolean in this section.
Let $S$ be a finite set,
$\mybf{B}S$ be a semi-module over the boolean with basis $S$.
A tensor product of $\mybf{B}S$ is called Kleen algebra of $S$,
and we denote $K_{\mybf{B}}S$.

\begin{definition}[Brzozowski derivative]
Let $S$ be a finite set and $KS=(KS, +, 0, \cdot, 1)$ be a Kleene algebra of $S$.
Brzozowski derivate $\Delta_B:KS\to KS\otimes KS$ is defined recursively.
For all $\alpha,\beta\in KS$
\begin{equation}\begin{split}
	\Delta_B(\alpha + \beta) &=  \Delta_B(\alpha) + \Delta_B(\beta) \\
	\Delta_B(\alpha \beta) &= \Delta_B(\alpha) \beta + \delta_B(\alpha) \Delta_B(\beta)\\
\end{split}\end{equation}
, and for all $a\in S$
\begin{equation}\begin{split}
	\Delta_B(a) &=  a\otimes 1 \\
	\Delta_B(0) &=  0 \\
	\Delta_B(1) &=  0 \\
\end{split}\end{equation}
. Where $\delta_B:KS\to \myop{boolean}$ is define as the followings:
\begin{equation}\begin{split}
	\delta_B(\alpha) &= \begin{cases}
		1 & \text{iff there exists } \beta\in KS \text{ sucth that } \alpha = 1 + \beta \\
		0 & \text{otherwise} \\
		\end{cases} \\
\end{split}\end{equation}
.
\end{definition}

The followings are examples of the Brzozowski derivative:
\begin{equation}\begin{split}
	\Delta_B(ab) &= a\otimes b \\
	\Delta_B(abc) &= a\otimes bc \\
	\Delta_B(a^*) &= a\otimes a^* \\
	\Delta_B(\alpha^2) &= \Delta_B(\alpha) \alpha + \delta_B(\alpha) \Delta_B(\alpha) \\
	\Delta_B(\alpha^*) &= \Delta_B(\alpha) \alpha^* \\
\end{split}\end{equation}
.

The subset construction of automaton theory can be defined in terms of 
Brzozowski derivative.

\begin{definition}[Subset construction]
Let $S$ be a finite set, $KS=(KS, +, 0, \cdot, 1)$ be the Kleene algebra of $S$,
$\Delta_B$ be the Brzozowski derivative on $KS$.
Let $K^2S$ be a free semi-module of $KS$ over the $\myop{boolean}$,
$K^2S=\myop{boolean}+KS+KS\otimes KS +\cdots$.
The subset construction $\Gamma_B$ is the following map:
\begin{equation}\begin{split}
	\Gamma_B: KS &\to K^2S \\
		\Gamma_B &= \myid + \Delta_B + \Delta_B^2 + \cdots \\
		\Delta_B^{n+1} &= \kakko{\underbrace{\myid\otimes \cdots \otimes \myid}_{n\text{ times}} \otimes \Delta_B} \circ \Delta_B^n \\
\end{split}\end{equation}
.
\end{definition}

\subsection{CSV file format}
Let consider the following BNF, Backus Nauer Format:
\begin{equation}\begin{split}\label{eq:csv.bnf}
	\bnfword{L} &= \bnfletter{S}^*\bnfword{F}\kakko{1+\bnfletter{C}\bnfword{L}} \\
	\end{split}\end{equation}
, 

\subsection{Segment approximation}
We consider the case which both source and target spaces are 1-dimension.
We denote $F$ as $F=\set{\mybf{R}\to\mybf{R}}$.

\subsection{Backup}
This section is a note for the article of the scale space theory at wikipedia
\url{http://en.wikipedia.org/wiki/Scale-space|Scale space}.

\begin{definition}[Scale space axiom]
We denote $F$ as $F=\set{\mybf{R}\to\mybf{R}}$.
The map $g\in\set{\mybf{R}\times F\to F}$ is called begin satisfied 
scale-space axiom, if $g$ is satisfied the following properties:
\begin{itemize}
\item linearlity
\begin{equation}\begin{split}
	g(t,f_1+f_2) &= g(t,f_1)+g(t,f_2) \\
	g(t,af) &= ag(t,f) \\
\end{split}\end{equation}
\item translation invariant
Let $\gamma_a$ for all $a\in\mybf{R}$ be the followings:
\begin{equation}\begin{split}
	\gamma_a: F &\to F \\
		f &\mapsto \gamma_a(f) \text{ such that } \gamma_a(f)(x) = f(x+a) \\ 
\end{split}\end{equation}
. $g$ satisfies the followings commutative diagram for all $a\in\mybf{R}$:
\begin{equation}\xymatrix{
	F \ar[r]^{\gamma_a} \ar[d]^{g} & F \ar[d]^{g} \\
	F \ar[r]^{\gamma_a} & F \\
}\end{equation}
.
\end{itemize}
\end{definition}

\subsection{Try and Error}
We denote the followings:
\begin{itemize}
\item $I_R$ as the interval $\set{x\in\mybf{R}\bou \zettai{x}< R}$,
\item $F_R$ as the set of maps $\set{I_R\to \mybf{R}}$,
\item $G$ as a Lie group on $\mybf{R}$,
\item $\beta$ as the map
\begin{equation}\begin{split}
	\beta:G\times F_R\times F_R &\to \mybf{R} \\
		(g,f_1,f_2) &\mapsto \int_{x\in g(I_R)\cap I_R} (f_1\circ g)(x)f_2(x) \\
\end{split}\end{equation}
\end{itemize}
. The map $\beta$ is quasi-symmetric $\beta(g,f_1,f_2)=\beta(g^{-1},f_2,f_1)$.
\begin{proof}
We get the following equation by changing integration variable.
\begin{equation}\begin{split}
	\beta(g,f_1,f_2) &= \int_{x\in g(I_R)\cap I_R} (f_1\circ g)(x)f_2(x) \\
		&= \int_{y\in I_R)\cap g^{-1}(I_R)} f_1(y)(f_2\circ g^{-1})(y) \\
		&= \beta(g^{-1},f_2,f_1) \\
\end{split}\end{equation}
\end{proof}
We define the map $\beta_G$ as the followings:
\begin{equation}\begin{split}
	\beta:F_R\times F_R &\to \mybf{R} \\
		(f_1,f_2) &\mapsto \int_{g\in G} \beta(g,f_1,f_2) \\
\end{split}\end{equation}
. The amp $\beta_G$ is symmetric $\beta_G(f_1,f_2)=\beta_G(f_2,f_1)$.

Let examine in the case of $G$ being the translation on $\mybf{R}$.
Let $g_a:x\mapsto x+a$. The integration region is given as the followings:
\begin{equation}\begin{split}
	g_a((I_R)\cap I_R &= \set{x\in\mybf{R}\bou \zettai{x+a}< R \text{ and } \zettai{x}< R} \\
	&= \begin{cases}
		\emptyset & \text{case } 2R\le \zettai{a} \\
		\bakko{-R, R-a} & \text{case } 0\le a\le 2R \\
		\bakko{-R-a, R} & \text{case } -2R\le a\le 0 \\
		\end{cases} \\
\end{split}\end{equation}
. Then 
\begin{equation}\begin{split}
	\beta(g_a,f_1,f_2) &= \begin{cases}
		0 & \text{case } 2R\le \zettai{a} \\
		\int_{x=-R}^{R-a}f_1(x+a)f_2(x) & \text{case } 0\le a\le 2R \\
		\int_{x=-R-a}^{R}f_1(x+a)f_2(x) & \text{case } -2R\le a\le 0 \\
		\end{cases} \\
	\beta_G(f_1,f_2) &= \kakko{\int_{a=0}^{2R}\int_{x=-R}^{R-a}+\int_{a=-2R}^{0}\int_{x=-R-a}^{R}}f_1(x+a)f_2(x) \\
	&= \int_{x=-R}^{R}\int_{a=-R-x}^{R-x}f_1(x+a)f_2(x) \\
\end{split}\end{equation}

\subsection{Backup}
Let consider the following gaussian function.
\begin{equation}\begin{split}\label{eq:f.gauss}
	\gamma(t,f,g) &= \frac{1}{\sqrt{2\pi t}}\intallr{a}\exp\kakko{-\frac{d(a, f,g)}{2t}} \\
	d(a,f,g) &= \intallr{x}\kakko{f\kakko{x+a}-g\kakko{x}}^2 \\
		&= \beta(0,f,f) + \beta(0,g,g) - 2\beta(a,f,g) \\
	\beta(a,f,g) &= \intallr{x}f(x+a)g(x) \\
\end{split}\end{equation}
Of course, the function $\gamma(t,f,g)$ is not probability density w.r.t. the variable $f$.
We shall consider the mesure in the later.
Let examine $\gamma$ for the boxcar functions.
We define the boxcar function as the followings.\footnote {
	This definition of the boxcar function may differ from the standard definition.
}
\begin{equation}\begin{split}
	h_r(x) &= \begin{cases}
		1 & \text{iff} -r\le x< r \\
		0 & \text{else} \\
		\end{cases}\\
\end{split}\end{equation}
We get the followiings for the boxcar functions $h_r$ and $h_s$.
\begin{equation}\begin{split}
	\beta(a,h_r,h_s) &= \intallr{x}(-r\le x+a< r)\text{ and }(-s\le x< s) \\
		&= \begin{cases}
			0 & \text{case } r+s\le\zettai{a} \\
			r+s-\zettai{a} & \text{case } \zettai{r-s}\le\zettai{a}\le r+s \\
			2\min(r,s) & \text{case } \zettai{a}\le\zettai{r-s} \\
		\end{cases}\\
\end{split}\end{equation}
, then
\begin{equation}\begin{split}
	d(a,h_r,h_s) &= 2(r+s) - 2\beta(a,h_r,h_s) \\
		&= \begin{cases}
			2(r+s) & \text{case } r+s\le\zettai{a} \\
			2\zettai{a} & \text{case } \zettai{r-s}\le\zettai{a}\le r+s \\
			2\zettai{r-s} & \text{case } \zettai{a}\le\zettai{r-s} \\
			\end{cases}
\end{split}\end{equation}
, then owing to symmetricity $d(a,\dots)=d(-a,\dots)$
\begin{equation}\begin{split}
	\gamma(t,h_r,h_s) 
		&= \sqrt{\frac{2}{\pi t}}\int_{a=0}^{\infty}\exp\kakko{-\frac{d(a,h_r,h_s)}{2t}} \\ 
	\sqrt{\frac{\pi t}{2}}\gamma(t,h_r,h_s) 
		&= \int_{a=0}^{\infty}\exp\kakko{-\frac{d(a,h_r,h_s)}{2t}} \\ 
		&= \int_{a=0}^{\zettai{r-s}}\exp\kakko{-\frac{\zettai{r-s}}{t}}
			+ \int_{a=\zettai{r-s}}^{r+s}\exp\kakko{-\frac{a}{t}}
			+ \int_{a=r+s}^{\Lambda} \exp\kakko{-\frac{r+s}{t}} \\
		&= \zettai{r-s}\exp\kakko{-\frac{\zettai{r-s}}{t}} \\
			&\quad + t\kakko{\exp\kakko{-\frac{\zettai{r-s}}{t}} - \exp\kakko{-\frac{r+s}{t}}} \\
			&\quad + \kakko{\Lambda - \kakko{r+s}} \exp\kakko{-\frac{r+s}{t}} \\
\end{split}\end{equation}
, where $\Lambda$ is cutoff paramter of translation group $\mybf{R}$.
This result can be summarized as the followings.
\begin{equation}\begin{split}
	\sqrt{\frac{\pi}{2}}\gamma(t,h_r,h_s) 
		&= \alpha_1(\zettai{r-s}, t) - \alpha_1(r+s, t) + \frac{\Lambda}{\sqrt{t}}\exp\kakko{-\frac{r+s}{t}} \\
	\alpha_1(x, t) &= \kakko{\frac{x}{\sqrt{t}} + \sqrt{t}}\exp\kakko{-\frac{x}{t}} \\
\end{split}\end{equation}
The variable $t$ in the formula \eqref{eq:f.gauss}  is a variance of normal
distribution. Since we do not know the apriori variance, we sum all variances
with the following formula.
\begin{equation}\begin{split}
	\varphi(s,f,g) &= \int_{t=0}^{\infty}\gamma(t,f,g)\eta(s,t) \\
	\eta(s,t) &= s\exp(-st) \\
\end{split}\end{equation}
If $\gamma(t,f,g)$ is normalized for all $t$, e.g.
\begin{equation}\begin{split}
	1 &= \int_{f\in\set{\rmapr}}\mu(f)\gamma(t,f,g) \\
\end{split}\end{equation}
, $\varphi(s,f,g)$ is also normalized.
Anyway, $\varphi(s,h_r,h_s)$ can be written as the followings.
\begin{equation}\begin{split}
	\varphi(t_0,h_r,h_s) &= \sqrt{\frac{2}{\pi}}\int_{t=0}^{t_0}f(t_0,t,h_r,h_s) \\
	f(t_0,t,h_r,h_s) &= \alpha_2(\zettai{r-s},t_0,t) - \alpha_2(r+s,t_0,t) + \frac{\Lambda}{\sqrt{t}}\exp\kakko{-\kakko{\frac{r+s}{t}+t_0t}} \\
	\alpha_2(x,s,t) &= \kakko{\frac{x}{\sqrt{t}} + \sqrt{t}}\exp\kakko{-\kakko{\frac{x}{t}+st}} \\
\end{split}\end{equation}
\begin{todo}[symmetry]
The function $\alpha_2$ in the above formula is seemed to have symmetry w.r.t.
$x$ and $t$ when $s=1$. Check it.
\end{todo}

\subsection{Convolution}
We use the standard notation to represent the image of map. 
For example we denote $g(x)f(x)$ instead of $*(f,g)(x,x)$.

Let $\rmapr$ be the set of all maps from $\mybf{R}$ to $\mybf{R}$.
We can define the structure of ring on $\rmapr$ from the target space 
ring structure $(\mybf{R},+,0,*,1)$.
\begin{equation}\begin{split}
	(f+g)x &= f(x)+g(x) \\
	(f*g)x &= f(x)*g(x) \\
\end{split}\end{equation}
We denote $\loner\subset\rmapr$ as the space of the followings:
\begin{equation}\begin{split}
	\loner &= \set{f\in\rmapr\bou \int_{x\in\mybf{R}}\zettai{f(x)} < \infty}
\end{split}\end{equation}
. The $\loner$ is closed under $+$ and $*$.
\begin{equation}\begin{split}
	\intallr{x}(f+g)(x) &= \kakko{\intallr{x}f(x)}+ \kakko{\intallr{x}g(x)} \\
		&\le \kakko{\intallr{x}\zettai{f(x)}}+ \kakko{\intallr{x}\zettai{g(x)}} \\
		& < \infty \\
	\intallr{x}(f*g)x &= \kakko{\intallr{x}f(x)g(x)} \\
		&\le \kakko{\intallr{x}\zettai{f(x)}}* \kakko{\intallr{x}\zettai{g(x)}} \\
		& < \infty \\
\end{split}\end{equation}
Where we use the following inequality to derive the second inequality.
\begin{equation}\begin{split}
	(a*b)+(c*d) 
		&\le \zettai{a*b}+\zettai{c*d} \\
		&\le \kakko{\zettai{a}+\zettai{c}}*\kakko{\zettai{b}+\zettai{d}} \\
		&\text{for all }a,b,c,d\in\mybf{R} \\
\end{split}\end{equation}
Note that all non-zero constant maps are not elements of $\loner$,
especially $1\not\in\loner$. 
Therefore, $*$ dose not have identity $1$ in the $\loner$.
The lack of identity is owing to non-compactness of base manifold.
If we consider the space $\loner_\Lambda$:
\begin{equation}\begin{split}
	\loner_\Lambda = \set{f\in\loner\bou f(x)=0 \text{ for all }\Lambda<\zettai{x}}
\end{split}\end{equation}
. All constant maps $\set{c*h(\Lambda,x)}_{c\in\myop{R}}$ are elements of 
$\loner_\Lambda$, and $h(\Lambda,x)$ is an identity of $*$.
Where $h$ is the boxcar function:
\begin{equation}\begin{split}\label{eq:boxcar}
	h: \myop{R}\times\myop{R} &\to \myop{R} \\
	(r, x) &\mapsto \begin{cases}
		1 & \text{iff }-r\le x<r \\
		0 & \text{otherwise} \\
		\end{cases} \\
\end{split}\end{equation}
. 
This definition of the boxcar function may differ from the standard definition.

We define another multiplication beside $*$.
\begin{definition}[Convolution]
We define the map $\sqcap$ as the followings:
\begin{equation}\begin{split}
	\sqcap: \rmapr\times\rmapr &\to \rmapr \\
	(f,g) &\mapsto (f\sqcap g) \text{ such that} \\
	& (f\sqcap g)(x) = \int_{y\in\mybf{R}}f(x-y)g(y) \\
\end{split}\end{equation}
. The map $\sqcap$ is called convolution.
\end{definition}
The convolution satisfies the folllowing properties:
\begin{itemize}
\item distritutive $f\sqcap(g+ h) = (f\sqcap g)+ (f\sqcap h)$
\item associative $(f\sqcap g)\sqcap h = f\sqcap(g\sqcap h)$
	\begin{proof}
		by changing variable of integration.
		\begin{equation*}\begin{split}
			\intallr{z}\kakko{f\sqcap g}\kakko{x-z}*\kakko{hz}
				&= \intallr{y}\intallr{z}f(x-y-z)*g(y)*h(z) \\
				&= \intallr{y}\intallr{z}f(x-y)*g(y-z)*h(z) \\
				&= \intallr{y}f(x-y)*(g\sqcap h)(y) \\
		\end{split}\end{equation*}
	\end{proof}
\item commutative $f\sqcap g = g\sqcap f$
	\begin{proof}
		by changing variable of integration.
		\begin{equation*}\begin{split}
			\intallr{y}f(x-y)*g(y) &= \intallr{y}f(y)*g(x-y) \\
		\end{split}\end{equation*}
	\end{proof}
\end{itemize}
.
The $\loner$ is closed also under $\sqcap$, and $\sqcap$ also dose not
have identity in the $\loner$.

Let examine $\sqcap$ for boxcar functions\eqref{eq:boxcar}.
\begin{equation}\begin{split}
	\kakko{h_{r_1}\sqcap h_{r_2}}x 
		&= \intallr{y}h_{r_1}(x-y)*h_{r_2}(y) \\
		&= \intallr{y}\kakko{-r_1+x\le y\le r_1+x}\text{ and }\kakko{-r_2\le y\le r_2} \\
\end{split}\end{equation}
The integration is given by the intersection of the ranges
$-r_1.x\le r_1+x$ and $-r_2\le r_2$, the intersection is given by the
shuffle product of these ranges.
\begin{equation}\begin{split}
	&\intallr{y}\kakko{-r_1+x\le y\le r_1+x}\text{ and }\kakko{-r_2\le y\le r_2} \\
	&=\begin{cases}
		-r_1+x\le r_1+x\le -r_2\le r_2 &\implies 0 \\
		-r_2\le r_2\le -r_1+x\le r_1+x &\implies 0 \\
		-r_1+x\le -r_2\le r_1+x\le r_2 &\implies r_1+r_2+x \\
		-r_2\le -r_1+x\le r_2\le r_1+x &\implies r_1+r_2-x \\
		-r_1+x\le -r_2\le r_2\le r_1+x &\implies 2*r_2 \\
		-r_2\le -r_1+x\le r_1+x\le r_2 &\implies 2*r_1 \\
		\end{cases} \\
	&=\begin{cases}
		r_1+r_2 \le  \zettai{x} &\implies 0 \\
		\zettai{r_1-r_2} \le \zettai{x} \le r_1+r_2 &\implies r_1+r_2-x \\
		\zettai{x} \le  \zettai{r_1-r_2} &\implies 2*\min(r_1,r_2) \\
		\end{cases} \\
\end{split}\end{equation}
Finally, we get the followings:
\begin{equation}\begin{split}
	\kakko{h_{r_1}\sqcap h_{r_2}}x 
		&=\begin{cases}
			0 & \text{case } r_1+r_2 \le \zettai{x} \\
			r_1+r_2-x & \text{case } \zettai{r_1-r_2} \le \zettai{x} \le r_1+r_2 \\
			2*\min(r_1,r_2) & \text{case } \zettai{x} \le  \zettai{r_1-r_2} \\
			\end{cases} \\
\end{split}\end{equation}
. 

\begin{observe}[Quantom deformation]
Let be $\mu$ a gaussin function:
\begin{equation}\begin{split}
	\mu:\mybf{R}\times\mybf{R} &\to \mybf{R} \\
		(t,x) &\mapsto \kakko{\frac{1}{2 \pi t}}^{\frac{1}{2}}\exp\kakko{-\frac{x^2}{2t}} \\
\end{split}\end{equation}
. We define the multiplication $\sqcap_t$ as the followings:
\begin{equation}\begin{split}\label{eq:t.conv}
	\sqcap_t: \rmapr\times\rmapr &\to \rmapr \\
	(f,g) &\mapsto (f\sqcap_t g) \text{ such that} \\
	& (f\sqcap_tg)(x) = \int_{y\in\mybf{R}}\mu(t,y)*f(x-y)*g(y) \\
\end{split}\end{equation}
. The multiplication $\sqcap$ is connected by $\sqcap_t$:
\begin{equation}\begin{split}\label{eq:conv.deform}
	f\sqcap g &= \lim_{t\to\infty}(f\sqcap_tg) \\
\end{split}\end{equation}
.
\begin{itemize}
\item Are \eqref{eq:conv.deform} true?
\item Dose \eqref{eq:t.conv} satisfies the leibnitz rule for differential?
\begin{equation}\begin{split}
	d(f\sqcap_tg) &= \kakko{\kakko{df}\sqcap_tg}+\kakko{f\sqcap_t\kakko{dg}}
\end{split}\end{equation}
\item Can we calculate \eqref{eq:t.conv} for general $t$?
\item Can we generalize \eqref{eq:t.conv} to gauge theory?
\end{itemize}
\end{observe}

\subsection{Order and Word}\label{sub:forget}
\begin{itemize}
	\item Consider the map $\myop{ord}$
	\begin{equation}\begin{split}
		\myop{ord}: F\mybf{Z} &\to \mybf{B} \\
		\bakko{a_1a_2\cdots a_n} &\mapsto a_1\le a_2\le\cdots\le a_n \text{ for all }2\le n \\
	\end{split}\end{equation}
	Where $\mybf{B}$ is a boolean $\set{0,1}$.
	\item $\myop{ord}$ is not homeo with word concatenation, because 
	$\myop{ord}\bakko{abcd}$ cannot be written with the combination of 
	$\myop{ord}\bakko{ab}$ and $\myop{ord}\bakko{cd}$.
	\item looking for multiplication which $\myop{ord}$ is homeo with.
	\item We know the shuffle product $\sqcup$.\footnote{
		We couldnt find suitable symbol for the shuffle product.
	}
	Let $F_{\mybf{B}}\mybf{Z}$ be a free-semimodule $\set{F\mybf{Z}\to\mybf{B}}$.
	We denote $+$ for locgical 'or', empty word for logical 'and'.
	We define $\myop{ord}$ to satisfy homeo with $+$.
	\begin{equation}\begin{split}
		\myop{ord}(w_1+w_2) &= (\myop{ord}w_1)+(\myop{ord}w_2) \text{ for all }w_1,w_2\in F\mybf{Z} \\
	\end{split}\end{equation}
	The condition homeo with $+$ implies $\myop{ord}0=0$.
	We can show the following equation by explicit calculation:
	\begin{equation}\begin{split}
		\myop{ord}(\bakko{ab}\sqcup\bakko{cd}) 
			&= (\myop{ord}\bakko{ab})\wedge(\myop{ord}\bakko{cd}) \text{ for all }a,b,c,d\in \mybf{Z} \\
	\end{split}\end{equation}
	. We can show the following equation:
	\begin{equation}\begin{split}
		\myop{ord}(w_1\sqcup w_2) &= (\myop{ord}w_1)\wedge(\myop{ord}w_2) \\
			& \text{ for all }w_1,w_2\in \mybf{Z}\text{ and }\zettai{w_1}\not=1 \text{ and }\zettai{w_2}\not=1 \\
	\end{split}\end{equation}
	. 
	The condition homeo with $\sqcup$ and $\wedge $implies $\myop{ord}\bakko{}=1$.
	And also implies the followings:
	\begin{equation}\begin{split}
		1 = \myop{ord}\sqcup(\bakko{a},\bakko{b}) 
			= (\myop{ord}\bakko{a}) \wedge (\myop{ord}\bakko{a})
	\end{split}\end{equation}
	. $\myop{ord}\bakko{a}=1$ is sufficient condition for $\myop{ord}$ being
	homeo, though we dont know whether this condition is nessary condition.
	\item We conclude that the map $\myop{ord}$ satisfies the following 
	commutative diagram:
	\begin{equation}\xymatrix@C+24pt{
		F_{\mybf{B}}\mybf{Z}\otimes F_{\mybf{B}}\mybf{Z} \ar[r]^{\myop{ord}\times\myop{ord}} \ar[d]^{\square_F}
		& \mybf{B}\otimes \mybf{B} \ar[d]^{\square}
		\\
		F_{\mybf{B}}\mybf{Z} \ar[r]^{\myop{ord}}
		& \mybf{B} 
		\\
	}\end{equation}
	. Where $\square_F=+/\sqcup$ and $\square=+/\wedge$
\end{itemize}

\begin{todo}[decomposition]
We need decompose a word to enumerate ordering condition.
For example, we would like deform like that:
\begin{equation}\begin{split}
	\myop{ord}\bakko{abc} 
		&= \wedge(\myop{ord}, \myop{ord})(\bakko{ab}, \bakko{bc}) \\
		&= \myop{ord}\sqcup(\bakko{ab}, \bakko{bc}) \\
\end{split}\end{equation}
How to justify with associative or co-associative operations?
\end{todo}
